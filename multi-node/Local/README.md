# ğŸ§ª Manual Multi-Node Training with NeMo (No Slurm / No K8s)

æœ¬æ•™å­¸èªªæ˜å¦‚ä½•ä½¿ç”¨æ‰‹å‹•æ–¹å¼åœ¨å¤šå€‹ç¯€é»ä¸Šé€²è¡Œ NeMo çš„æ¨¡å‹è¨“ç·´ï¼Œä¸ä¾è³´ Slurmã€Kubernetes ç­‰ä»»å‹™èª¿åº¦å·¥å…·ï¼Œå®Œå…¨ä»¥ NeMo-Run ä¸­çš„ Local Executor (torchrun) åœ¨å®¹å™¨ä¸­å•Ÿå‹•è·¨ç¯€é»è¨“ç·´ã€‚

é©åˆéœ€è¦éˆæ´»æ§åˆ¶ã€æ¸¬è©¦ç’°å¢ƒæˆ–å°å‹å¢é›†çš„ä½¿ç”¨è€…ã€‚

---

## ğŸ“¦ 1. å•Ÿå‹• NeMo å®¹å™¨ï¼ˆæ‰€æœ‰ç¯€é»çš†éœ€åŸ·è¡Œï¼‰

è«‹åœ¨**æ¯å€‹è¨ˆç®—ç¯€é»**ä¸ŠåŸ·è¡Œä»¥ä¸‹ Docker æŒ‡ä»¤ä»¥å•Ÿå‹•è¨“ç·´å®¹å™¨ï¼š

```bash
docker run \
    --gpus all -it --rm --shm-size=16g --ulimit memlock=-1 --ulimit stack=67108864 \
    -v $HOME:$HOME --network host \
    nvcr.io/nvidia/nemo:25.02
```

---

## ğŸ”§ 2. æ›´æ–° NeMo ç¨‹å¼ç¢¼èˆ‡å¥—ä»¶ï¼ˆæ‰€æœ‰ç¯€é»çš†éœ€åŸ·è¡Œï¼‰
åœ¨**æ¯å€‹è¨ˆç®—ç¯€é»**ä¸­çš„å®¹å™¨ä¸­åŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤ä»¥æ‹‰å–æœ€æ–°ç‰ˆæœ¬çš„ NeMo-Run ç¨‹å¼ç¢¼èˆ‡å®‰è£å¿…è¦å¥—ä»¶ï¼š

```bash
cd /opt/NeMo-Run/
git pull origin main
pip install toml
```

---

## ğŸ”½ 3. æ¨¡å‹ä¸‹è¼‰èˆ‡è½‰æ›ï¼ˆåƒ…ä¸»ç¯€é»åŸ·è¡Œï¼‰

### ç™»å…¥ Hugging Face

```bash
huggingface-cli login --token <HF_TOKEN>
```

### ä¸‹è¼‰æ¨¡å‹

ä»¥ `meta-llama/Llama-3.1-8B-Instruct` ç‚ºä¾‹ï¼ŒåŸ·è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è¼‰æ¨¡å‹ï¼š

```bash
huggingface-cli download meta-llama/Llama-3.1-8B-Instruct \
    --local-dir Llama-3.1-8B-Instruct \
    --local-dir-use-symlinks=False
```

### æ¨¡å‹è½‰æ›

Option 1: é€éPython
```bash
HF_MODEL_ID=Llama-3.1-8B-Instruct
OUTPUT_PATH=nemo_ckpt/Llama-3.1-8B-Instruct

python convert_ckpt/convert_from_hf_to_nemo.py \
  --source ${HF_MODEL_ID} \
  --output_path ${OUTPUT_PATH}
```

Option 2: é€éCli
```bash
MODEL=llama3_8b
HF_MODEL_ID=Llama-3.1-8B-Instruct
OUTPUT_PATH=nemo_ckpt/Llama-3.1-8B-Instruct
OVERWRITE_EXISTING=false

nemo llm import -y model=${MODEL} source=hf://${HF_MODEL_ID} output_path=${OUTPUT_PATH} overwrite=${OVERWRITE_EXISTING}
```

è«‹å°‡è½‰æ›å¥½çš„æ¨¡å‹æ”¾ç½®æ–¼æ‰€æœ‰ç¯€é»å…±äº«å¯å­˜å–çš„ä½ç½®ï¼ˆä¾‹å¦‚NFSï¼‰ã€‚

---

## ğŸ“š 4. è³‡æ–™ä¸‹è¼‰èˆ‡é è™•ç†ï¼ˆåƒ…ä¸»ç¯€é»åŸ·è¡Œï¼‰

ä¸‹è¼‰ç¯„ä¾‹è³‡æ–™é›†ä¸¦é€²è¡Œè™•ç†ï¼š

```python
from datasets import load_dataset

dataset = load_dataset('erhwenkuo/wikinews-zhtw')['train']
dataset.to_json('./data/custom_dataset/json/wikinews-zhtw.jsonl', force_ascii=False)
exit()
```

è³‡æ–™é è™•ç†ä»¥é©é… NeMo è¨“ç·´æ ¼å¼ï¼š

```bash
mkdir -p data/custom_dataset/preprocessed

python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \
    --input=data/custom_dataset/json/wikinews-zhtw.jsonl \
    --json-keys=text \
    --dataset-impl mmap \
    --tokenizer-library=huggingface \
    --tokenizer-type meta-llama/Llama-3.1-8B-Instruct \
    --output-prefix=data/custom_dataset/preprocessed/wikinews \
    --append-eod
```

åŒæ¨£åœ°ï¼Œé è™•ç†å¾Œçš„è³‡æ–™éœ€æ”¾ç½®åœ¨æ‰€æœ‰ç¯€é»å…±äº«å¯å­˜å–çš„ä½ç½®ã€‚

---

## ğŸŒ 5. å¤šç¯€é»ç’°å¢ƒè®Šæ•¸è¨­ç½®ï¼ˆä¾ç¯€é»åˆ†åˆ¥åŸ·è¡Œï¼‰
æ¯å°ç¯€é»çš†éœ€è¨­ç½®ä»¥ä¸‹ç’°å¢ƒè®Šæ•¸ã€‚æ³¨æ„ `NODE_RANK` éœ€æ ¹æ“šç¯€é»ç·¨è™Ÿåšèª¿æ•´ã€‚

ä»¥2å€‹ç¯€é»ç‚ºä¾‹ï¼š

### ğŸ–¥ï¸ ç¯€é» 1
```bash
export MASTER_ADDR=<ç¯€é» 1 çš„ä¸»æ©Ÿåç¨±/IPä½å€>
export MASTER_PORT=12345
export NODE_RANK=0
```

### ğŸ–¥ï¸ ç¯€é» 2
```bash
export MASTER_ADDR=<ç¯€é» 1 çš„ä¸»æ©Ÿåç¨±/IPä½å€>
export MASTER_PORT=12345
export NODE_RANK=1
```

ğŸ“Œ è‹¥æœ‰æ›´å¤šç¯€é»ï¼Œä¾åºå¢åŠ  `NODE_RANK`ï¼ˆç¯€é» 3 è¨­ç‚º 2ï¼Œç¯€é» 4 è¨­ç‚º 3ï¼Œä»¥æ­¤é¡æ¨ï¼‰ã€‚
`MASTER_ADDR` å¿…é ˆæ˜¯ç¯€é» 1 çš„ ä¸»æ©Ÿåç¨± / IP ä½å€ï¼Œæ‰€æœ‰ç¯€é»çš†éœ€ä¸€è‡´ã€‚

---

## âš™ï¸ 6. åŸ·è¡Œå¤šç¯€é»é è¨“ç·´ï¼ˆæ‰€æœ‰ç¯€é»çš†éœ€åŸ·è¡Œï¼‰
è¨­å®šç’°å¢ƒè®Šæ•¸å¾Œï¼Œæ–¼æ¯å€‹ç¯€é»ä¸ŠåŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼š

```bash
JOB_NAME=llama31_pretraining

NUM_NODES=2
NUM_GPUS=8

HF_MODEL_ID=Llama-3.1-8B-Instruct
NEMO_MODEL=/mnt/nemo_ckpt/${HF_MODEL_ID}
HF_TOKEN=<HF_TOKEN>

TP=2
PP=1
CP=1

GBS=2048
MAX_STEPS=100
DATASET_PATH=data/custom_dataset/preprocessed/

python pretraining/pretrain.py \
    --executor local \
    --experiment ${JOB_NAME} \
    --num_nodes ${NUM_NODES} \
    --num_gpus ${NUM_GPUS} \
    --model_size 8B \
    --hf_model_id meta-llama/${HF_MODEL_ID} \
    --nemo_model ${NEMO_MODEL} \
    --hf_token ${HF_TOKEN} \
    --max_steps ${MAX_STEPS} \
    --global_batch_size ${GBS} \
    --tensor_model_parallel_size ${TP} \
    --pipeline_model_parallel_size ${PP} \
    --context_parallel_size ${CP} \
    --dataset_path ${DATASET_PATH}
```

---

## ğŸ”§ 7. å¤šç¯€é»å¾®èª¿ï¼ˆæ‰€æœ‰ç¯€é»çš†éœ€åŸ·è¡Œï¼‰

ä¸‹è¼‰å¾®èª¿è³‡æ–™ï¼š

```bash
python finetuning/download_split_data.py
```

åŸ·è¡Œå¾®èª¿ï¼š

```bash
```bash
JOB_NAME=llama31_finetuning

NUM_NODES=2
NUM_GPUS=8

HF_MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
NEMO_MODEL= # [Optional]
HF_TOKEN=<HF_TOKEN>

TP=2
PP=1
CP=1

MAX_STEPS=100
GBS=128
DATASET_PATH=data/alpaca

python finetuning/finetune.py \
    --executor local \
    --experiment ${JOB_NAME} \
    --num_nodes ${NUM_NODES} \
    --num_gpus ${NUM_GPUS} \
    --model_size 8B \
    --hf_model_id ${HF_MODEL_ID} \
    --hf_token ${HF_TOKEN} \
    --nemo_model ${NEMO_MODEL} \
    --max_steps ${MAX_STEPS} \
    --global_batch_size ${GBS} \
    --tensor_model_parallel_size ${TP} \
    --pipeline_model_parallel_size ${PP} \
    --context_parallel_size ${CP} \
    --dataset_path ${DATASET_PATH}
```

---

## ğŸ“ é™„è¨»èˆ‡å»ºè­°
- æ‰€æœ‰ç¯€é»æ‡‰å¯äº’ç›¸é€šè¨Šï¼Œè«‹ç¢ºä¿é˜²ç«ç‰†èˆ‡ç¶²è·¯è¨­å®šå…è¨±ä½¿ç”¨ MASTER_PORTã€‚
- `NEMO_MODEL`, `DATASET_PATH` è·¯å¾‘éœ€ç¢ºä¿ç‚ºæ‰€æœ‰ç¯€é»å¯å­˜å–ï¼ˆå»ºè­°ä½¿ç”¨ NFS æˆ–å…±äº«ç£ç¢Ÿï¼‰ã€‚
- `HF_TOKEN` éœ€å…·å‚™ Hugging Face æ¬Šé™ä»¥å­˜å–æ¨¡å‹ã€‚