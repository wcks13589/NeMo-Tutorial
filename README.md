# ä½¿ç”¨ NVIDIA NeMo é€²è¡Œå¤§å‹èªè¨€æ¨¡å‹ (LLM) çš„è¨“ç·´ ğŸ¤–

æœ¬å°ˆæ¡ˆå°‡å¸¶æ‚¨å®Œæ•´é«”é©—ä½¿ç”¨ [NVIDIA NeMo](https://github.com/NVIDIA/NeMo) é€²è¡Œå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å®Œæ•´æµç¨‹ï¼Œå¾é›¶é–‹å§‹å­¸æœƒæ¨¡å‹è½‰æ›ã€é è¨“ç·´ã€å¾®èª¿åˆ°éƒ¨ç½²çš„å¯¦æˆ°æŠ€å·§ã€‚

## ğŸ¯ å­¸ç¿’ç›®æ¨™

é€šéæœ¬å°ˆæ¡ˆï¼Œæ‚¨å°‡å­¸æœƒï¼š

1. **ğŸ”„ æ¨¡å‹è½‰æ›æŠ€èƒ½**ï¼šæŒæ¡ Hugging Face èˆ‡ NeMo æ ¼å¼é–“çš„è½‰æ›
2. **ğŸ“š é è¨“ç·´å¯¦è¸**ï¼šé«”é©—å¤§è¦æ¨¡èªè¨€æ¨¡å‹çš„æŒçºŒé è¨“ç·´
3. **ğŸ› ï¸ å¾®èª¿æŠ€è¡“**ï¼šå­¸æœƒé‡å°ç‰¹å®šä»»å‹™é€²è¡Œæ¨¡å‹å¾®èª¿ã€æŒæ¡ LoRA ç­‰åƒæ•¸é«˜æ•ˆå¾®èª¿æ–¹æ³•
4. **ğŸ“Š æ¨¡å‹æ¨è«–**ï¼šå­¸æœƒæ¸¬è©¦æ¨¡å‹æ€§èƒ½
5. **ğŸš€ æ¨¡å‹è½‰æ›èˆ‡è©•ä¼°**ï¼šäº†è§£æ¨¡å‹å°å‡ºå’Œè©•ä¼°æ–¹æ³•

---

## ğŸ“‚ æ•™å­¸å¤§ç¶±

- [ğŸš€ é–‹å§‹ä¹‹å‰ï¼šç’°å¢ƒè¨­å®š](#ğŸ› ï¸-ç’°å¢ƒæº–å‚™)
  - [ğŸ“– è©³ç´°ç’°å¢ƒè¨­å®šæŒ‡å—](setup/README.md) â­
- [ğŸ“¥ å°ˆæ¡ˆè¨­ç½®](#ğŸ“¥-å°ˆæ¡ˆè¨­ç½®)
- [ğŸ“– è©³ç´°æ•™å­¸æ­¥é©Ÿ](#ğŸ“–-è©³ç´°æ•™å­¸æ­¥é©Ÿ)
  - [ç¬¬ä¸€ç« ï¼šæ¨¡å‹è½‰æ›åŸºç¤](#ç¬¬ä¸€ç« æ¨¡å‹è½‰æ›åŸºç¤)
  - [ç¬¬äºŒç« ï¼šæŒçºŒé è¨“ç·´å¯¦æˆ°](#ç¬¬äºŒç« æŒçºŒé è¨“ç·´å¯¦æˆ°)
  - [ç¬¬ä¸‰ç« ï¼šæŒ‡ä»¤å¾®èª¿æŠ€è¡“](#ç¬¬ä¸‰ç« æŒ‡ä»¤å¾®èª¿æŠ€è¡“)
  - [ç¬¬å››ç« ï¼šReasoning è³‡æ–™å¾®èª¿æŠ€è¡“](#ç¬¬å››ç« reasoning-è³‡æ–™å¾®èª¿æŠ€è¡“)
  - [ç¬¬äº”ç« ï¼šæ¨¡å‹è©•ä¼°èˆ‡æ¸¬è©¦](#ç¬¬äº”ç« æ¨¡å‹è©•ä¼°èˆ‡æ¸¬è©¦)
  - [ç¬¬å…­ç« ï¼šæ¨¡å‹éƒ¨ç½²èˆ‡è½‰æ›](#ç¬¬å…­ç« æ¨¡å‹éƒ¨ç½²èˆ‡è½‰æ›)
  - [ç¬¬ä¸ƒç« ï¼šæ¨™æº–åŒ–æ¨¡å‹è©•ä¼°](#ç¬¬ä¸ƒç« æ¨™æº–åŒ–æ¨¡å‹è©•ä¼°)
- [ğŸ’¡ å¯¦æˆ°æŠ€å·§](#ğŸ’¡-å¯¦æˆ°æŠ€å·§)
- [ğŸ“š é€²éšå­¸ç¿’è³‡æº](#ğŸ“š-é€²éšå­¸ç¿’è³‡æº)

---

## ğŸ› ï¸ ç’°å¢ƒæº–å‚™

### ğŸ“‹ é¸æ“‡æ‚¨çš„ç’°å¢ƒ

åœ¨é–‹å§‹è¨“ç·´ä¹‹å‰ï¼Œæ‚¨éœ€è¦è¨­å®šé©åˆçš„ GPU ç’°å¢ƒã€‚æˆ‘å€‘æä¾›è©³ç´°çš„ç’°å¢ƒè¨­å®šæŒ‡å—ï¼š

#### ğŸ³ æœ¬åœ° Docker ç’°å¢ƒ

ä½¿ç”¨å®˜æ–¹ NeMo å®¹å™¨ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„ä¾è³´å¥—ä»¶ï¼š

```bash
docker run \
    --gpus all -it --rm --shm-size=8g --ulimit memlock=-1 --ulimit stack=67108864 \
    -v $PWD:$PWD -w $PWD -p 8888:8888 \
    nvcr.io/nvidia/nemo:25.04
```

> ğŸ’¡ **æç¤º**ï¼šæ‚¨å¯ä»¥åœ¨ [NGC NeMo Catalog](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo/tags) æŸ¥çœ‹æœ€æ–°ç‰ˆæœ¬

---

## ğŸ“¥ å°ˆæ¡ˆè¨­ç½®

### ä¸‹è¼‰æ•™å­¸å°ˆæ¡ˆ

```bash
git clone https://github.com/wcks13589/NeMo-Tutorial.git
cd NeMo-Tutorial
```

> ğŸ’¡ **æç¤º**ï¼šè«‹ç¢ºä¿åœ¨ `NeMo-Tutorial` å°ˆæ¡ˆç›®éŒ„ä¸­åŸ·è¡Œå¾ŒçºŒæŒ‡ä»¤ã€‚

### ğŸ”‘ è¨­å®š Hugging Face æ¬Šé™

ç”³è«‹ä¸¦è¨­å®šæ‚¨çš„ Hugging Face Tokenï¼š

1. **ç”³è«‹ Token**ï¼šå‰å¾€ [Hugging Face Settings](https://huggingface.co/settings/tokens) å»ºç«‹æ–°çš„ Access Token
2. **è¨­å®šç’°å¢ƒè®Šæ•¸**ï¼š
   ```bash
   # æ›¿æ›ç‚ºæ‚¨çš„å¯¦éš› Token
   export HF_TOKEN="your_hf_token"
   huggingface-cli login --token $HF_TOKEN
   ```

> ğŸ“Œ **é‡è¦**ï¼šè«‹å…ˆåœ¨ [Hugging Face](https://huggingface.co/settings/tokens) ç”³è«‹ Access Token

## ğŸ“– è©³ç´°æ•™å­¸æ­¥é©Ÿ

### ç¬¬ä¸€ç« ï¼šæ¨¡å‹è½‰æ›åŸºç¤

#### 1.1 ä¸‹è¼‰é è¨“ç·´æ¨¡å‹

```bash
# ä¸‹è¼‰ Llama 3.1 8B Instruct æ¨¡å‹
huggingface-cli download meta-llama/Llama-3.1-8B-Instruct \
    --local-dir Llama-3.1-8B-Instruct \
    --exclude original/
```

#### 1.2 è½‰æ›ç‚º NeMo æ ¼å¼

**Option 1:** ä½¿ç”¨Pythonè…³æœ¬

```bash
# è¨­å®šè®Šæ•¸
HF_MODEL_ID=Llama-3.1-8B-Instruct
OUTPUT_PATH=nemo_ckpt/Llama-3.1-8B-Instruct

python convert_ckpt/convert_from_hf_to_nemo.py \
  --source ${HF_MODEL_ID} \
  --output_path ${OUTPUT_PATH}
```

**Option 2:** ä½¿ç”¨CLI
```bash
# è¨­å®šè®Šæ•¸
MODEL=llama3_8b
HF_MODEL_ID=Llama-3.1-8B-Instruct
OUTPUT_PATH=nemo_ckpt/Llama-3.1-8B-Instruct
OVERWRITE_EXISTING=false

# åŸ·è¡Œè½‰æ›
nemo llm import -y \
    model=${MODEL} \
    source=hf://${HF_MODEL_ID} \
    output_path=${OUTPUT_PATH} \
    overwrite=${OVERWRITE_EXISTING}
```

> âœ… **æª¢æŸ¥é»**ï¼šç¢ºèª `nemo_ckpt/` ç›®éŒ„ä¸‹æˆåŠŸç”Ÿæˆäº† NeMo æ ¼å¼çš„æ¨¡å‹æª”æ¡ˆ

---

### ç¬¬äºŒç« ï¼šæŒçºŒé è¨“ç·´å¯¦æˆ°

#### 2.1 æº–å‚™è¨“ç·´è³‡æ–™

**ä¸‹è¼‰ä¸­æ–‡è³‡æ–™é›†**ï¼š

```bash
python data_preparation/download_pretrain_data.py \
    --dataset_name erhwenkuo/wikinews-zhtw \
    --output_dir data/custom_dataset/json/wikinews-zhtw.jsonl
```

#### 2.2 è³‡æ–™é è™•ç†

```bash
# å»ºç«‹é è™•ç†ç›®éŒ„
mkdir -p data/custom_dataset/preprocessed

# ä½¿ç”¨ NeMo çš„è³‡æ–™é è™•ç†å·¥å…·
python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \
    --input=data/custom_dataset/json/wikinews-zhtw.jsonl \
    --json-keys=text \
    --dataset-impl mmap \
    --tokenizer-library=huggingface \
    --tokenizer-type meta-llama/Llama-3.1-8B-Instruct \
    --output-prefix=data/custom_dataset/preprocessed/wikinews \
    --append-eod
```

#### 2.3 åŸ·è¡Œé è¨“ç·´

##### å‰ç½®æº–å‚™

è¨­å®šåŸºæœ¬åƒæ•¸ï¼š

```bash
JOB_NAME=llama31_pretraining
NUM_NODES=1
NUM_GPUS=8
HF_MODEL_ID=meta-llama/Llama-3.1-8B-Instruct

# å¹³è¡Œè™•ç†åƒæ•¸
TP=2  # Tensor Parallel
PP=1  # Pipeline Parallel  
CP=1  # Context Parallel

# è¨“ç·´åƒæ•¸
GBS=4          # Global Batch Size
MAX_STEPS=20  # æœ€å¤§è¨“ç·´æ­¥æ•¸(æ¨¡å‹æ¬Šé‡æ›´æ–°æ¬¡æ•¸)
DATASET_PATH=data/custom_dataset/preprocessed/
```

##### æ–¹æ³•ä¸€ï¼šå¾é ­é–‹å§‹é è¨“ç·´æ¨¡å‹

**é©ç”¨æƒ…æ³**ï¼šç•¶æ‚¨æƒ³è¦å¾é›¶é–‹å§‹è¨“ç·´æ¨¡å‹æ™‚ä½¿ç”¨ã€‚

**ç‰¹é»**ï¼šè…³æœ¬æœƒè‡ªå‹•å¾åŸºç¤æ¨¡å‹æ¶æ§‹é€²è¡Œæ¬Šé‡åˆå§‹åŒ–

**åŸ·è¡ŒæŒ‡ä»¤**ï¼š
```bash
python pretraining/pretrain.py \
   --executor local \
   --experiment ${JOB_NAME} \
   --num_nodes ${NUM_NODES} \
   --num_gpus ${NUM_GPUS} \
   --model_size 8B \
   --hf_model_id ${HF_MODEL_ID} \
   --hf_token ${HF_TOKEN} \
   --max_steps ${MAX_STEPS} \
   --global_batch_size ${GBS} \
   --tensor_model_parallel_size ${TP} \
   --pipeline_model_parallel_size ${PP} \
   --context_parallel_size ${CP} \
   --dataset_path ${DATASET_PATH}
```

##### æ–¹æ³•äºŒï¼šå¾é è¨“ç·´æ¨¡å‹é–‹å§‹ç¹¼çºŒé è¨“ç·´

**é©ç”¨æƒ…æ³**ï¼šç•¶æ‚¨æƒ³è¦å¾ç¾æœ‰çš„ NeMo æ ¼å¼æ¨¡å‹é–‹å§‹ï¼Œé€²è¡ŒæŒçºŒé è¨“ç·´æ™‚ä½¿ç”¨ã€‚

**å‰ç½®æ¢ä»¶**ï¼š
- éœ€è¦å…ˆå°‡ Hugging Face æ¨¡å‹è½‰æ›ç‚º NeMo æ ¼å¼
- ç¢ºä¿ `${NEMO_MODEL}` è·¯å¾‘ä¸‹å­˜åœ¨æœ‰æ•ˆçš„ NeMo æ¨¡å‹æª”æ¡ˆ

**åŸ·è¡ŒæŒ‡ä»¤**ï¼š
```bash
NEMO_MODEL=nemo_ckpt/Llama-3.1-8B-Instruct

python pretraining/pretrain.py \
   --executor local \
   --experiment ${JOB_NAME} \
   --num_nodes ${NUM_NODES} \
   --num_gpus ${NUM_GPUS} \
   --model_size 8B \
   --hf_model_id ${HF_MODEL_ID} \
   --nemo_model ${NEMO_MODEL} \
   --hf_token ${HF_TOKEN} \
   --max_steps ${MAX_STEPS} \
   --global_batch_size ${GBS} \
   --tensor_model_parallel_size ${TP} \
   --pipeline_model_parallel_size ${PP} \
   --context_parallel_size ${CP} \
   --dataset_path ${DATASET_PATH}
```

---

### ç¬¬ä¸‰ç« ï¼šæ¨¡å‹å¾®èª¿ (Fine-tuning)

#### 3.1 æº–å‚™å¾®èª¿è³‡æ–™

```bash
# ä¸‹è¼‰ä¸¦æº–å‚™ Alpaca è³‡æ–™é›†
python data_preparation/download_sft_data.py
```

#### 3.2 åŸ·è¡Œæ¨¡å‹å¾®èª¿

```bash
# å¾®èª¿åƒæ•¸è¨­å®š
JOB_NAME=llama31_finetuning
NUM_NODES=1
NUM_GPUS=8
HF_MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
# NEMO_MODEL=nemo_ckpt/Llama-3.1-8B-Instruct
NEMO_MODEL=$(find nemo_experiments/llama31_pretraining/checkpoints/ -type d -name "*-last" | sort -r | head -n 1)
HF_TOKEN=$HF_TOKEN

# å¹³è¡Œè™•ç†åƒæ•¸
TP=2
PP=1
CP=1

# å¾®èª¿åƒæ•¸
MAX_STEPS=10
GBS=4
DATASET_PATH=data/alpaca

# åŸ·è¡Œ LoRA å¾®èª¿
python finetuning/finetune.py \
    --executor local \
    --experiment ${JOB_NAME} \
    --num_nodes ${NUM_NODES} \
    --num_gpus ${NUM_GPUS} \
    --model_size 8B \
    --hf_model_id ${HF_MODEL_ID} \
    --hf_token ${HF_TOKEN} \
    --nemo_model ${NEMO_MODEL} \
    --max_steps ${MAX_STEPS} \
    --global_batch_size ${GBS} \
    --tensor_model_parallel_size ${TP} \
    --pipeline_model_parallel_size ${PP} \
    --context_parallel_size ${CP} \
    --dataset_path ${DATASET_PATH}
#    --peft lora
```

> ğŸ¯ **é«˜æ•ˆåƒæ•¸åƒæ•¸å¾®èª¿**ï¼šè‹¥è¦é€²è¡Œé«˜æ•ˆåƒæ•¸çš„å¾®èª¿ï¼Œè«‹æ–°å¢`--peft lora`

---

### ç¬¬å››ç« ï¼šReasoning è³‡æ–™å¾®èª¿æŠ€è¡“
> ğŸ§  **Reasoning å¾®èª¿ç‰¹è‰²**ï¼šä½¿ç”¨é«˜å“è³ªçš„æ¨ç†è³‡æ–™é›†ï¼Œæå‡æ¨¡å‹çš„é‚è¼¯æ¨ç†å’Œè¤‡é›œå•é¡Œè§£æ±ºèƒ½åŠ›

#### 4.1 æº–å‚™ Reasoning è³‡æ–™é›†

```bash
# å»ºç«‹ Reasoning è³‡æ–™é›†ç›®éŒ„
mkdir -p data/reasoning_dataset/

# ä¸‹è¼‰ NVIDIA Llama-Nemotron å¾Œè¨“ç·´è³‡æ–™é›†
wget https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset/resolve/main/SFT/chat/chat.jsonl -P data/reasoning_dataset/

# å¾è³‡æ–™é›†ä¸­é¸å–æ¨£æœ¬é€²è¡Œå¿«é€Ÿè¨“ç·´
head -n 200 data/reasoning_dataset/chat.jsonl > data/reasoning_dataset/chat_subset.jsonl
```

#### 4.2 è³‡æ–™é è™•ç†èˆ‡ç­–å±•

```bash
export UCX_MEMTYPE_CACHE=n
export UCX_TLS=tcp

# åŸ·è¡Œè³‡æ–™ç­–å±•èˆ‡é è™•ç†
python data_preparation/curate_reasoning_data.py \
    --input-dir "data/reasoning_dataset" \
    --filename-filter "chat_subset" \
    --remove-columns "category" "generator" "license" "reasoning" "system_prompt" "used_in_training" "version" \
    --json-files-per-partition 16 \
    --tokenizer "meta-llama/Llama-3.1-8B-Instruct" \
    --max-token-count 16384 \
    --max-completion-token-count 8192 \
    --output-dir data/reasoning_dataset/curated-data \
    --device "gpu" \
    --n-workers 1
```

> ğŸ’¡ **åŸ·è¡Œæç¤º**ï¼šæ­¤ç¨‹å¼åŸ·è¡Œéç¨‹ä¸­å¯èƒ½æœƒå‡ºç¾ä¸€äº›éŒ¯èª¤è¨Šæ¯ï¼Œä½†åªè¦è¼¸å‡ºè³‡æ–™å¤¾ `data/reasoning_dataset/curated-data` å…§æœ‰æª”æ¡ˆç”¢ç”Ÿå°±ç®—åŸ·è¡ŒæˆåŠŸï¼Œå¯ä»¥å¿½ç•¥éŒ¯èª¤è¨Šæ¯ç¹¼çºŒå¾ŒçºŒæ­¥é©Ÿã€‚

#### 4.3 åŸ·è¡Œ Reasoning å¾®èª¿

```bash
# Reasoning å¾®èª¿åƒæ•¸è¨­å®š
JOB_NAME=llama31_reasoning_finetuning
NUM_NODES=1
NUM_GPUS=8
HF_MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
# NEMO_MODEL=nemo_ckpt/Llama-3.1-8B-Instruct
NEMO_MODEL=$(find nemo_experiments/llama31_finetuning/checkpoints/ -type d -name "*-last" | sort -r | head -n 1)
HF_TOKEN=$HF_TOKEN

# å¹³è¡Œè™•ç†åƒæ•¸
TP=2
PP=1
CP=1

# å¾®èª¿åƒæ•¸
MAX_STEPS=10
GBS=4
DATASET_PATH=data/reasoning_dataset/curated-data

# åŸ·è¡Œ Reasoning å¾®èª¿
python finetuning/finetune.py \
    --executor local \
    --experiment ${JOB_NAME} \
    --num_nodes ${NUM_NODES} \
    --num_gpus ${NUM_GPUS} \
    --model_size 8B \
    --hf_model_id ${HF_MODEL_ID} \
    --hf_token ${HF_TOKEN} \
    --nemo_model ${NEMO_MODEL} \
    --max_steps ${MAX_STEPS} \
    --global_batch_size ${GBS} \
    --tensor_model_parallel_size ${TP} \
    --pipeline_model_parallel_size ${PP} \
    --context_parallel_size ${CP} \
    --dataset_path ${DATASET_PATH}
```
> ğŸ¯ **é«˜æ•ˆåƒæ•¸åƒæ•¸å¾®èª¿**ï¼šè‹¥è¦é€²è¡Œé«˜æ•ˆåƒæ•¸çš„å¾®èª¿ï¼Œè«‹æ–°å¢`--peft lora`

---

### ç¬¬äº”ç« ï¼šæ¨¡å‹è©•ä¼°èˆ‡æ¸¬è©¦

#### 5.1 æº–å‚™æ¸¬è©¦è³‡æ–™

```bash
# å¾æ¸¬è©¦é›†ä¸­é¸å–æ¨£æœ¬é€²è¡Œå¿«é€Ÿè©•ä¼°
head -n 30 data/alpaca/test.jsonl > data/alpaca/test_subset.jsonl
```

#### 5.2 åŸ·è¡Œæ¨ç†æ¸¬è©¦

```bash
# ä½¿ç”¨å¾®èª¿å¾Œçš„æ¨¡å‹é€²è¡Œæ¨ç†
# æ‰¾åˆ°æœ€æ–°çš„æª¢æŸ¥é»è³‡æ–™å¤¾
LATEST_CHECKPOINT=$(find nemo_experiments/llama31_reasoning_finetuning/checkpoints/ -type d -name "*-last" | sort -r | head -n 1)

python evaluation/inference.py \
    --peft_ckpt_path ${LATEST_CHECKPOINT} \
    --input_dataset data/alpaca/test_subset.jsonl \
    --output_path data/alpaca/peft_prediction.jsonl
```

#### 5.3 è¨ˆç®—è©•ä¼°æŒ‡æ¨™

```bash
# è¨ˆç®—æ¨¡å‹æ€§èƒ½æŒ‡æ¨™
python /opt/NeMo/scripts/metric_calculation/peft_metric_calc.py \
    --pred_file data/alpaca/peft_prediction.jsonl \
    --label_field "label" \
    --pred_field "prediction"
```

---

### ç¬¬å…­ç« ï¼šæ¨¡å‹è½‰æ›

> âš ï¸ **é‡è¦æé†’**ï¼šå¦‚æœæ‚¨é€²è¡Œçš„æ˜¯ LoRA å¾®èª¿ï¼Œè«‹å…ˆåŸ·è¡Œæ­¥é©Ÿ 6.1 åˆä½µ LoRA æ¬Šé‡ï¼Œå†é€²è¡Œæ­¥é©Ÿ 6.2 çš„æ ¼å¼è½‰æ›ã€‚

#### 6.1 åˆä½µ LoRA æ¬Šé‡ï¼ˆåƒ…é™ LoRA å¾®èª¿ï¼‰

å¦‚æœæ‚¨ä½¿ç”¨äº† LoRA é€²è¡Œå¾®èª¿ï¼ˆåœ¨å¾®èª¿æŒ‡ä»¤ä¸­åŒ…å« `--peft lora`ï¼‰ï¼Œæ‚¨éœ€è¦å…ˆå°‡ LoRA æ¬Šé‡åˆä½µå›åŸºåº•æ¨¡å‹ï¼Œç„¶å¾Œå†é€²è¡Œæ ¼å¼è½‰æ›ï¼š

```bash
# æ‰¾åˆ°æœ€æ–°çš„ LoRA checkpoint
LATEST_LORA_CHECKPOINT=$(find nemo_experiments/llama31_reasoning_finetuning/checkpoints/ -type d -name "*-last" | sort -r | head -n 1)
NEMO_MODEL=nemo_experiments/llama31_reasoning_finetuning/checkpoints/nemo_ckpt_merged

# åˆä½µ LoRA æ¬Šé‡åˆ°åŸºåº•æ¨¡å‹
python finetuning/merge_lora.py \
    --nemo_lora_model ${LATEST_LORA_CHECKPOINT} \
    --output_path ${NEMO_MODEL}
```

> ğŸ’¡ **èªªæ˜**ï¼š
> - æ­¤æ­¥é©Ÿæœƒå°‡ LoRA é©é…å™¨çš„æ¬Šé‡åˆä½µåˆ°åŸå§‹çš„åŸºåº•æ¨¡å‹ä¸­
> - åˆä½µå¾Œçš„æ¨¡å‹åŒ…å«å®Œæ•´çš„æ¬Šé‡ï¼Œå¯ä»¥ç¨ç«‹ä½¿ç”¨
> - å¦‚æœæ‚¨é€²è¡Œçš„æ˜¯å…¨åƒæ•¸å¾®èª¿ï¼Œè«‹è·³éæ­¤æ­¥é©Ÿ

#### 6.2 è½‰æ›å› Hugging Face æ ¼å¼

```bash
# è¨­å®šè½‰æ›åƒæ•¸
# å¦‚æœæ‚¨é€²è¡Œçš„æ˜¯å…¨åƒæ•¸å¾®èª¿ï¼Œè«‹ä½¿ç”¨ï¼š
NEMO_MODEL=$(find nemo_experiments/llama31_reasoning_finetuning/checkpoints/ -type d -name "*-last" | sort -r | head -n 1)
# å¦‚æœæ‚¨å®Œæˆäº† LoRA åˆä½µï¼Œè«‹ä½¿ç”¨åˆä½µå¾Œçš„æ¨¡å‹è·¯å¾‘ï¼š
# NEMO_MODEL=nemo_experiments/llama31_reasoning_finetuning/checkpoints/nemo_ckpt_merged

OUTPUT_PATH=hf_ckpt/

# åŸ·è¡Œè½‰æ›
nemo llm export -y \
    path=${NEMO_MODEL} \
    output_path=${OUTPUT_PATH} \
    target=hf
```

---

### ç¬¬ä¸ƒç« ï¼šæ¨™æº–åŒ–æ¨¡å‹è©•ä¼°

#### 7.1 å®‰è£è©•ä¼°å·¥å…·

ä½¿ç”¨ EleutherAI çš„ lm-evaluation-harness å·¥å…·é€²è¡Œæ¨™æº–åŒ–æ¨¡å‹è©•ä¼°ï¼š

```bash
# ä¸‹è¼‰ä¸¦å®‰è£ lm-evaluation-harness
git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness
cd lm-evaluation-harness
pip install -e .
```

#### 7.2 åŸ·è¡Œæ¨™æº–åŒ–è©•ä¼°

ä½¿ç”¨ LAMBADA OpenAI ä»»å‹™è©•ä¼°æ¨¡å‹çš„èªè¨€å»ºæ¨¡èƒ½åŠ›ï¼š

```bash
# åˆ‡æ›å›ä¸»ç›®éŒ„
cd ..

# åŸ·è¡Œ LAMBADA OpenAI è©•ä¼° (åƒ…ä½¿ç”¨è¼ƒå°‘æ¨£æœ¬é€²è¡Œå¿«é€Ÿè©•ä¼°)
lm_eval --model hf \
    --model_args pretrained=hf_ckpt/ \
    --tasks lambada_openai \
    --device cuda:0 \
    --batch_size 8 \
    --limit 100
```

**åŸ·è¡Œçµæœç¯„ä¾‹**ï¼š

```
|    Tasks     |Version|Filter|n-shot|  Metric  |   |Value |   |Stderr|
|--------------|------:|------|-----:|----------|---|-----:|---|-----:|
|lambada_openai|      1|none  |     0|acc       |â†‘  |0.7100|Â±  |0.0456|
|              |       |none  |     0|perplexity|â†“  |3.4032|Â±  |0.5080|
```

**çµæœæŒ‡æ¨™èªªæ˜**ï¼š
- **acc (æº–ç¢ºç‡)**ï¼šæ¨¡å‹æ­£ç¢ºé æ¸¬å¥å­æœ€å¾Œä¸€å€‹è©çš„æ¯”ä¾‹ï¼Œæœ¬ä¾‹ç‚º 71%
- **perplexity (å›°æƒ‘åº¦)**ï¼šè¡¡é‡æ¨¡å‹å°æ–‡æœ¬çš„ä¸ç¢ºå®šæ€§ï¼Œæ•¸å€¼è¶Šä½è¶Šå¥½

#### 7.3 å…¶ä»–è©•ä¼°ä»»å‹™

æ‚¨ä¹Ÿå¯ä»¥å˜—è©¦å…¶ä»–å¸¸è¦‹çš„è©•ä¼°ä»»å‹™ï¼š

```bash
lm_eval --model hf \
    --model_args pretrained=hf_ckpt/ \
    --tasks arc_challenge \
    --device cuda:0 \
    --batch_size 8
```

> ğŸ“Š **è©•ä¼°ä»»å‹™èªªæ˜**ï¼š
> - **LAMBADA OpenAI**ï¼šæ¸¬è©¦èªè¨€å»ºæ¨¡å’Œä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ï¼Œè©•ä¼°æ¨¡å‹é æ¸¬å¥å­æœ€å¾Œä¸€å€‹è©çš„æº–ç¢ºæ€§
> - **ARC (AI2 Reasoning Challenge)**ï¼šæ¸¬è©¦ç§‘å­¸æ¨ç†èƒ½åŠ›
> 
> ğŸ”§ **èª¿å„ªæç¤º**ï¼š
> - å¯æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´ `batch_size` åƒæ•¸
> - ä½¿ç”¨ `--limit` åƒæ•¸é€²è¡Œå¿«é€Ÿæ¸¬è©¦
> - è©³ç´°çš„è©•ä¼°çµæœæœƒé¡¯ç¤ºæº–ç¢ºç‡å’Œå…¶ä»–ç›¸é—œæŒ‡æ¨™

---

## ğŸ“š é€²éšå­¸ç¿’è³‡æº

### å®˜æ–¹æ–‡æª”
- [NeMo å®˜æ–¹æ–‡ä»¶](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/)
- [NeMo GitHub](https://github.com/NVIDIA/NeMo)

### é€²éšä¸»é¡Œ
1. **å¤šæ¨¡æ…‹æ¨¡å‹è¨“ç·´**
2. **åˆ†æ•£å¼è¨“ç·´å„ªåŒ–**
3. **æ¨¡å‹å£“ç¸®èˆ‡é‡åŒ–**
4. **è‡ªå®šç¾©è³‡æ–™è¼‰å…¥å™¨**

---

## ğŸ‰ æ­å–œå®Œæˆ

é€šéæœ¬æ•™å­¸ï¼Œæ‚¨å·²ç¶“æŒæ¡äº†ï¼š
- âœ… å¤§å‹èªè¨€æ¨¡å‹çš„å®Œæ•´è¨“ç·´æµç¨‹
- âœ… NeMo æ¡†æ¶çš„æ ¸å¿ƒåŠŸèƒ½
- âœ… å¯¦éš›çš„ AI æ¨¡å‹é–‹ç™¼æŠ€èƒ½
- âœ… ä¼æ¥­ç´š AI æ‡‰ç”¨é–‹ç™¼åŸºç¤

**ä¸‹ä¸€æ­¥å»ºè­°**ï¼š
1. å˜—è©¦ä½¿ç”¨è‡ªå·±çš„è³‡æ–™é›†
2. æ¢ç´¢ä¸åŒçš„æ¨¡å‹æ¶æ§‹
3. å­¸ç¿’æ¨¡å‹éƒ¨ç½²èˆ‡æœå‹™åŒ–
4. åƒèˆ‡é–‹æºå°ˆæ¡ˆè²¢ç»

---

> ğŸ’¬ **éœ€è¦å¹«åŠ©ï¼Ÿ** æ­¡è¿åœ¨ [Issues](https://github.com/wcks13589/NeMo-Tutorial/issues) ä¸­æå‡ºå•é¡Œæˆ–å»ºè­°ï¼

**Happy Learning! ğŸš€**