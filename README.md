# ä½¿ç”¨ NVIDIA NeMo é€²è¡Œå¤§å‹èªè¨€æ¨¡å‹ (LLM) çš„è¨“ç·´ ğŸ¤–

æœ¬ç ”ç©¶å°ˆæ¡ˆæ—¨åœ¨ç³»çµ±æ€§åœ°é—¡è¿°å¦‚ä½•åˆ©ç”¨ [NVIDIA NeMo](https://github.com/NVIDIA/NeMo) å¹³å°å®Œæˆå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è½‰æ› ğŸ”„ã€é è¨“ç·´ ğŸ“š åŠå¾®èª¿ ğŸ› ï¸ æµç¨‹ã€‚ä»¥ä¸‹å…§å®¹æ¶µè“‹çš„å››å€‹ä¸»è¦éšæ®µåŒ…æ‹¬ï¼š

1. **æ¨¡å‹è½‰æ›ï¼šå¾ Huggingface æ ¼å¼åŒ¯å…¥è‡³ NeMo æ ¼å¼** ğŸ”„
2. **é è¨“ç·´ï¼ˆPretrainingï¼‰** ğŸ“–
3. **å¾®èª¿ï¼ˆFinetuningï¼‰** âœ¨
4. **æ¨¡å‹è½‰æ›ï¼šå¾ NeMo æ ¼å¼åŒ¯å‡ºè‡³ Huggingface æ ¼å¼** ğŸ”ƒ

æ¯å€‹éšæ®µå‡å°æ‡‰ç¨ç«‹çš„ Python è…³æœ¬ ğŸï¼Œç¢ºä¿åŠŸèƒ½æ¨¡çµ„åŒ–ã€‚

---

## ğŸ“‚ ç›®éŒ„ ğŸ“–

- [âš™ï¸ ç’°å¢ƒè¨­å®š](#âš™ï¸-ç’°å¢ƒè¨­å®š)
- [ğŸ“¥ å–å¾—è…³æœ¬](#ğŸ“¥-å–å¾—è…³æœ¬)
- [ğŸ› ï¸ æ“ä½œæµç¨‹](#ğŸ› ï¸-æ“ä½œæµç¨‹)
  - [1ï¸âƒ£ æ¨¡å‹è½‰æ›ï¼šå¾ Huggingface åŒ¯å…¥è‡³ NeMo](#1ï¸âƒ£-æ¨¡å‹è½‰æ›å¾-huggingface-åŒ¯å…¥è‡³-nemo)
  - [2ï¸âƒ£ é è¨“ç·´ï¼ˆPretrainingï¼‰](#2ï¸âƒ£-é è¨“ç·´pretraining)
  - [3ï¸âƒ£ å¾®èª¿ï¼ˆFinetuningï¼‰](#3ï¸âƒ£-å¾®èª¿finetuning)
  - [4ï¸âƒ£ æ¨¡å‹è½‰æ›ï¼šå¾ NeMo åŒ¯å‡ºè‡³ Huggingface](#4ï¸âƒ£-æ¨¡å‹è½‰æ›å¾-nemo-åŒ¯å‡ºè‡³-huggingface)
- [ğŸ“š åƒè€ƒè³‡æ–™](#ğŸ“š-åƒè€ƒè³‡æ–™)

---

## âš™ï¸ ç’°å¢ƒè¨­å®š ğŸ–¥ï¸

NVIDIA NeMo å®¹å™¨æœƒéš¨ NeMo ç‰ˆæœ¬æ›´æ–°åŒæ­¥ç™¼å¸ƒï¼Œæ‚¨å¯ä»¥åœ¨ [NeMo ç‰ˆæœ¬ç™¼å¸ƒé é¢](https://github.com/NVIDIA/NeMo/releases) æŸ¥è©¢æ›´å¤šè³‡è¨Šã€‚

### ä½¿ç”¨é æ§‹å»ºå®¹å™¨ï¼ˆæ¨è–¦ï¼‰

æ­¤æ–¹å¼é©ç”¨æ–¼å¸Œæœ›ä½¿ç”¨ç©©å®šç‰ˆæœ¬ NeMo çš„å¤§å¤šæ•¸ä½¿ç”¨è€…ã€‚è«‹æŒ‰ç…§ä»¥ä¸‹æ­¥é©Ÿæ“ä½œï¼š

1. ç™»éŒ„æˆ–å…è²»è¨»å†Šä¸€å€‹å¸³æˆ¶ï¼š [NVIDIA GPU Cloud (NGC)](https://ngc.nvidia.com/signin)ã€‚

2. ç™»éŒ„å¾Œï¼Œæ‚¨å¯ä»¥åœ¨ [NVIDIA NGC NeMo Framework](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo/tags) æŸ¥çœ‹æ‰€æœ‰å®¹å™¨ç‰ˆæœ¬ã€‚

3. åœ¨çµ‚ç«¯ä¸­åŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤å•Ÿå‹•å®¹å™¨ï¼š

```bash
docker run \
    --gpus all -it --rm --shm-size=8g --ulimit memlock=-1 --ulimit stack=67108864 \
    -v $PWD:$PWD -w $PWD -p 8888:8888 \
    nvcr.io/nvidia/nemo:25.02
```

æ­¤å®¹å™¨åŒ…å«æ‰€æœ‰æ‰€éœ€çš„æ ¸å¿ƒä¾è³´å¥—ä»¶ï¼ŒåŒ…æ‹¬ NeMoã€PyTorch å’Œå…¶ä»–ç›¸é—œå·¥å…·ã€‚è«‹ç¢ºä¿æ‚¨çš„è…³æœ¬å’Œè³‡æ–™å·²æ›è¼‰åˆ°å®¹å™¨å…§ä»¥é€²è¡Œå¾ŒçºŒæ“ä½œã€‚

---

## ğŸ“¥ å–å¾—è…³æœ¬

åœ¨æ“ä½œæµç¨‹ä¹‹å‰ï¼Œè«‹å…ˆåŸ·è¡Œä»¥ä¸‹å‘½ä»¤ä»¥ä¸‹è¼‰æœ¬å°ˆæ¡ˆçš„ç¨‹å¼åº«ã€‚

```bash
git clone https://github.com/wcks13589/NeMo-Tutorial.git
cd NeMo-Tutorial
```

---

## ğŸ› ï¸ æ“ä½œæµç¨‹ âš™ï¸

### 1ï¸âƒ£ æ¨¡å‹è½‰æ›ï¼šå¾ Huggingface åŒ¯å…¥è‡³ NeMo ğŸ”„

åœ¨æ­¤æ­¥é©Ÿä¸­ï¼Œéœ€å°‡ Huggingface æ ¼å¼çš„æ¨¡å‹æ¬Šé‡è½‰æ›ç‚º NeMo æ ¼å¼ã€‚

åœ¨åŸ·è¡Œè½‰æ›è…³æœ¬ä¹‹å‰ï¼Œè«‹å…ˆç¢ºä¿å·²ç¶“ç™»å…¥ Huggingface å¸³æˆ¶ä¸¦ä¸‹è¼‰æ¨¡å‹ã€‚

#### ç™»å…¥ Huggingface
åŸ·è¡Œä»¥ä¸‹å‘½ä»¤ä»¥å®Œæˆç™»å…¥ï¼š

```bash
huggingface-cli login --token <HF_TOKEN>
```

#### ä¸‹è¼‰æ¨¡å‹
ä»¥ `meta-llama/Llama-3.1-8B-Instruct` ç‚ºä¾‹ï¼ŒåŸ·è¡Œä»¥ä¸‹å‘½ä»¤ä»¥ä¸‹è¼‰æ¨¡å‹ï¼š

```bash
huggingface-cli download meta-llama/Llama-3.1-8B-Instruct --local-dir Llama-3.1-8B-Instruct --local-dir-use-symlinks=False
```

#### æ¨¡å‹è½‰æ›
ç™»å…¥ä¸¦ä¸‹è¼‰æ¨¡å‹å¾Œï¼Œæ‚¨å³å¯é€²è¡Œæ¨¡å‹è½‰æ›ã€‚åŸ·è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

Option 1: é€éPython
```bash
HF_MODEL_ID=Llama-3.1-8B-Instruct
OUTPUT_PATH=nemo_ckpt/Llama-3.1-8B-Instruct

python convert_ckpt/convert_from_hf_to_nemo.py \
  --source ${HF_MODEL_ID} \
  --output_path ${OUTPUT_PATH}
```

Option 2: é€éCli
```bash
MODEL=llama3_8b
HF_MODEL_ID=Llama-3.1-8B-Instruct
OUTPUT_PATH=nemo_ckpt/Llama-3.1-8B-Instruct
OVERWRITE_EXISTING=false

nemo llm import -y model=${MODEL} source=hf://${HF_MODEL_ID} output_path=${OUTPUT_PATH} overwrite=${OVERWRITE_EXISTING}
```

### 2ï¸âƒ£ æŒçºŒé è¨“ç·´ï¼ˆContinual Pretrainingï¼‰ ğŸ“–

#### è³‡æ–™ä¸‹è¼‰èˆ‡è™•ç† ğŸ—‚ï¸

åœ¨é€²è¡Œé è¨“ç·´ä¹‹å‰ï¼Œéœ€ä¸‹è¼‰ä¸¦è™•ç†ç›¸æ‡‰çš„èªæ–™æ•¸æ“šã€‚

ä¸‹è¼‰è³‡æ–™é›†ï¼š

```python
from datasets import load_dataset

dataset = load_dataset('erhwenkuo/wikinews-zhtw')['train']
dataset.to_json('./data/custom_dataset/json/wikinews-zhtw.jsonl', force_ascii=False)
exit()
```

é è™•ç†è³‡æ–™ä»¥é©é… NeMo æ ¼å¼ï¼š

```bash
mkdir -p data/custom_dataset/preprocessed

python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \
    --input=data/custom_dataset/json/wikinews-zhtw.jsonl \
    --json-keys=text \
    --dataset-impl mmap \
    --tokenizer-library=huggingface \
    --tokenizer-type meta-llama/Llama-3.1-8B-Instruct \
    --output-prefix=data/custom_dataset/preprocessed/wikinews \
    --append-eod
```

#### é è¨“ç·´éç¨‹

åˆ©ç”¨å¤§è¦æ¨¡èªæ–™å°æ¨¡å‹é€²è¡Œèªè¨€å»ºæ¨¡è¨“ç·´ï¼Œä»¥æå‡å…¶æ³›åŒ–èƒ½åŠ›ã€‚

åŸ·è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
JOB_NAME=llama31_pretraining

NUM_NODES=1
NUM_GPUS=8

HF_MODEL_ID=Llama-3.1-8B-Instruct
NEMO_MODEL=nemo_ckpt/${HF_MODEL_ID}
HF_TOKEN=<HF_TOKEN>

TP=2
PP=1
CP=1

GBS=2048
MAX_STEPS=100
DATASET_PATH=data/custom_dataset/preprocessed/

python pretraining/pretrain.py \
    --executor local \
    --experiment ${JOB_NAME} \
    --num_nodes ${NUM_NODES} \
    --num_gpus ${NUM_GPUS} \
    --model_size 8B \
    --hf_model_id meta-llama/${HF_MODEL_ID} \
    --nemo_model ${NEMO_MODEL} \
    --hf_token ${HF_TOKEN} \
    --max_steps ${MAX_STEPS} \
    --global_batch_size ${GBS} \
    --tensor_model_parallel_size ${TP} \
    --pipeline_model_parallel_size ${PP} \
    --context_parallel_size ${CP} \
    --dataset_path ${DATASET_PATH}
```

æ›´å¤šæ¬²è¨“ç·´çš„è©³ç´°åƒæ•¸è³‡è¨Šï¼Œè«‹æŸ¥é–±`pretraining`è³‡æ–™å¤¾å…§çš„èªªæ˜æ–‡ä»¶ ğŸ“„ã€‚

### æŒ‡ä»¤å¾®èª¿ (Instruction Tuning)

##### è³‡æ–™ä¸‹è¼‰ ğŸ—‚ï¸

åœ¨é€²è¡Œå¾®èª¿ä¹‹å‰ï¼Œéœ€ä¸‹è¼‰ä¸¦è™•ç†ç›¸æ‡‰çš„èªæ–™è³‡æ–™ã€‚

ä¸‹è¼‰è³‡æ–™é›†ï¼š

```bash
python finetuning/download_split_data.py
```

é‡å°å…·é«”ä»»å‹™æˆ–æŒ‡ä»¤èªè¨€é€²è¡Œæ¨¡å‹å¾®èª¿ã€‚åŸ·è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
JOB_NAME=llama31_finetuning

NUM_NODES=1
NUM_GPUS=8

HF_MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
NEMO_MODEL= # [Optional]
HF_TOKEN=<HF_TOKEN>

TP=2
PP=1
CP=1

MAX_STEPS=100
GBS=128
DATASET_PATH=data/alpaca

python finetuning/finetune.py \
    --executor local \
    --experiment ${JOB_NAME} \
    --num_nodes ${NUM_NODES} \
    --num_gpus ${NUM_GPUS} \
    --model_size 8B \
    --hf_model_id ${HF_MODEL_ID} \
    --hf_token ${HF_TOKEN} \
    --nemo_model ${NEMO_MODEL} \
    --max_steps ${MAX_STEPS} \
    --global_batch_size ${GBS} \
    --tensor_model_parallel_size ${TP} \
    --pipeline_model_parallel_size ${PP} \
    --context_parallel_size ${CP} \
    --dataset_path ${DATASET_PATH}
```

æ›´å¤šå¾®èª¿çš„è©³ç´°åƒæ•¸è³‡è¨Šï¼Œè«‹æŸ¥é–±`finetuning`è³‡æ–™å¤¾å…§çš„èªªæ˜æ–‡ä»¶ ğŸ“„ã€‚

### 4ï¸âƒ£ æ¨¡å‹è½‰æ›ï¼šå¾ NeMo åŒ¯å‡ºè‡³ Huggingface ğŸ”ƒ

æœ€å¾Œï¼Œå°‡ç¶“éè¨“ç·´æˆ–å¾®èª¿çš„ NeMo æ ¼å¼æ¨¡å‹è½‰æ›ç‚º Huggingface æ ¼å¼ï¼Œä»¥ä¾¿å¾ŒçºŒçš„å…¼å®¹æ€§æˆ–éƒ¨ç½²éœ€æ±‚ã€‚

åŸ·è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

Option 1: é€éPython
```bash
NEMO_MODEL=results/llama31_finetuning/checkpoints/model_name\=0--val_loss\=1.55-step\=9-consumed_samples\=80.0-last/
OUTPUT_PATH=hf_ckpt

python convert_ckpt/convert_from_nemo_to_hf.py \
  --source ${NEMO_MODEL} \
  --output_path ${OUTPUT_PATH}
```

Option 2: é€éCli
```bash
NEMO_MODEL=results/llama31_finetuning/checkpoints/model_name\=0--val_loss\=1.38-step\=99-consumed_samples\=1600.0-last/
OUTPUT_PATH=hf_ckpt

nemo llm export -y path=${NEMO_MODEL} output_path=${OUTPUT_PATH} target=hf
```

---

## ğŸ“š åƒè€ƒè³‡æ–™ ğŸ“˜

- [NVIDIA NeMo å®˜æ–¹æ–‡ä»¶](https://docs.nvidia.com/nemo-framework/user-guide/latest/overview.html) ğŸ“„