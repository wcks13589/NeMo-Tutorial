{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27af7c62",
   "metadata": {},
   "source": [
    "# TensorRT-LLM\n",
    "The objective of this notebook is to demonstrate the use of TensorRT-LLM to optimize Llama-3.1-8B-Instruct, run inference, and examine using various advance optimization techniques.\n",
    "\n",
    "## Overview of TensorRT-LLM\n",
    "\n",
    "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. It also includes a backend for integration with the NVIDIA Triton Inference Server. Models built with TensorRT-LLM can be executed on a wide range of configurations going from a single GPU to multiple nodes with multiple GPUs (using Tensor Parallelism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a86cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git clone https://github.com/NVIDIA/TensorRT-LLM.git -b v0.17.0 --single-branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92dc35a",
   "metadata": {},
   "source": [
    "## 1. Download model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fcf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "huggingface-cli login --token <HF_TOKEN>\n",
    "huggingface-cli download meta-llama/Llama-3.1-8B-Instruct --local-dir Llama-3.1-8B-Instruct --local-dir-use-symlinks=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e219a",
   "metadata": {},
   "source": [
    "## 2. Building TensorRT-LLM engine(s) for Llama-3.1-8B-Instruct\n",
    "\n",
    "This section shows how to build tensorrt engine(s) using huggingface model.\n",
    "Before we proceed to build our engine, it is important to be aware of the supported matrixes for Llama-3 as listed below:\n",
    "\n",
    "- FP16\n",
    "- FP8\n",
    "- INT8 & INT4 Weight-Only\n",
    "- SmoothQuant\n",
    "- Groupwise quantization (AWQ/GPTQ)\n",
    "- FP8 KV cache\n",
    "- INT8 KV cache (+ AWQ/per-channel weight-only)\n",
    "- Tensor Parallel\n",
    "\n",
    "### 2.1 Build TensorRT-LLM engines - BF16\n",
    "\n",
    "**TensorRT-LLM** builds TensorRT engine(s) from HF checkpoint. Firstly, we used the `convert_checkpoint.py` script to convert Llama-3-Taiwan-8B-Instruct into tensorrt-llm checkpoint format. We use the `trtllm-build` command to build our tensorrt engine.\n",
    "\n",
    "The `trtllm-build` command builds TensorRT-LLM engines from TensorRT-LLM checkpoints. The checkpoint directory provides the model's weights and architecture configuration. The number of engine files is also same to the number of GPUs used to run inference.\n",
    "\n",
    "`trtllm-build` command has a variety of options. In particular, the plugin-related options have two categories:\n",
    "\n",
    "- Plugin options that requires a data type (e.g., `gpt_attention_plugin`), you can\n",
    "    - explicitly specify `float16`/`bfloat16`/`float32`, so that the plugins are enabled with the specified precision;\n",
    "    - implicitly specify `auto`, so that the plugins are enabled with the precision automatically inferred from model dtype (i.e., the dtype specified in weight conversion); or\n",
    "    - disable the plugin by `disable`.\n",
    "    \n",
    "- Other features that requires a boolean (e.g., `context_fmha`, `paged_kv_cache`, `remove_input_padding`), you can\n",
    "enable/disable the feature by specifying `enable`/`disable`.\n",
    "\n",
    "Normally `trtllm-build` only requires single GPU, but if you've already got all the GPUs needed for inference, you could enable parallel building to make the engine building process faster by adding --workers argument. Please note that currently workers feature only supports single node.\n",
    "\n",
    "The last step is to run the inference using the `run.py` and `summarize.py` script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d81687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Define model weight path, output checkpoint path and output engine path\n",
    "HF_MODEL=Llama-3.1-8B-Instruct\n",
    "CKPT_PATH=ckpt/bf16\n",
    "ENGINE_PATH=llama31/bf16\n",
    "\n",
    "python3 TensorRT-LLM/examples/llama/convert_checkpoint.py \\\n",
    "    --model_dir $HF_MODEL \\\n",
    "    --output_dir $CKPT_PATH \\\n",
    "    --dtype bfloat16 \\\n",
    "    --tp_size 1\n",
    "\n",
    "trtllm-build \\\n",
    "    --checkpoint_dir $CKPT_PATH \\\n",
    "    --output_dir $ENGINE_PATH \\\n",
    "    --gemm_plugin auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f5d329",
   "metadata": {},
   "source": [
    "#### flag description for `convert_checkpoint.py`:\n",
    "- `model_dir`: path to the model directory\n",
    "- `output_dir`: path to the directory to store the tensorrt-llm checkpoint format or the tensorrt engine\n",
    "- `dtype`: data type to use for model conversion to tensorrt-llm checkpoint\n",
    "\n",
    "#### flag description for `trtllm-build`:\n",
    "- `checkpoint_dir`: path to the directory to load the tensorrt-llm checkpoint needed to build the tensorrt engine\n",
    "- `output_dir`: path to the directory to store the tensorrt-llm checkpoint format or the tensorrt engine\n",
    "- `gemm_plugin`: required plugin to prevent accuracy issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667c412",
   "metadata": {},
   "source": [
    "### 2.2 Build TensorRT-LLM engines - INT8 KV cache + per-channel weight-only quantization\n",
    "To maximize performance and reduce memory footprint, TensorRT-LLM allows the models to be executed using different quantization modes. TensorRT-LLM supports INT4 or INT8 weights (and FP16 activations; a.k.a. INT4/INT8 weight-only) as well as a complete implementation of the SmoothQuant technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d28e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install datasets==2.19\n",
    "\n",
    "# Define model weight path, output checkpoint path and output engine path\n",
    "HF_MODEL=Llama-3.1-8B-Instruct\n",
    "CKPT_PATH=ckpt/int8\n",
    "ENGINE_PATH=llama31/int8\n",
    "\n",
    "python3 TensorRT-LLM/examples/llama/convert_checkpoint.py \\\n",
    "    --model_dir $HF_MODEL \\\n",
    "    --output_dir $CKPT_PATH \\\n",
    "    --dtype bfloat16 \\\n",
    "    --tp_size 1 \\\n",
    "    --int8_kv_cache \\\n",
    "    --use_weight_only \\\n",
    "    --weight_only_precision int8\n",
    "\n",
    "trtllm-build \\\n",
    "    --checkpoint_dir $CKPT_PATH \\\n",
    "    --output_dir $ENGINE_PATH \\\n",
    "    --gemm_plugin auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30e439",
   "metadata": {},
   "source": [
    "### 2.3 Build TensorRT-LLM engines - FP8 Post-Training Quantization [Optional]\n",
    "\n",
    "The examples below uses the NVIDIA Modelopt (AlgorithMic Model Optimization) toolkit for the model quantization process. Although the V100 does not support the FP8 datatype, we have included it as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model weight path, output checkpoint path and output engine path\n",
    "HF_MODEL=Llama-3.1-8B-Instruct\n",
    "CKPT_PATH=ckpt/fp8\n",
    "ENGINE_PATH=llama31/fp8\n",
    "\n",
    "python3 TensorRT-LLM/examples/quantization/quantize.py \\\n",
    "    --model_dir $HF_MODEL \\\n",
    "    --dtype bfloat16 \\\n",
    "    --qformat fp8 \\\n",
    "    --kv_cache_dtype fp8 \\\n",
    "    --output_dir $CKPT_PATH \\\n",
    "    --calib_size 512 \\\n",
    "    --tp_size 1\n",
    "\n",
    "\n",
    "trtllm-build --checkpoint_dir $CKPT_PATH \\\n",
    "             --output_dir $ENGINE_PATH \\\n",
    "             --gemm_plugin auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c7c40",
   "metadata": {},
   "source": [
    "### 2.4 Build TensorRT-LLM engines - Groupwise quantization (AWQ/GPTQ)\n",
    "One can enable AWQ/GPTQ INT4 weight only quantization with these options when building engine with trtllm-build:\n",
    "NVIDIA Modelopt toolkit is used for AWQ weight quantization. Please see [examples/quantization/README.md](tensorrtllm_backend/tensorrt_llm/examples/quantization/README.md) for Modelopt installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model weight path, output checkpoint path and output engine path\n",
    "HF_MODEL=Llama-3.1-8B-Instruct\n",
    "CKPT_PATH=ckpt/int4_awq\n",
    "ENGINE_PATH=llama31/int4_awq\n",
    "\n",
    "# Quantize HF LLaMA 8B checkpoint into INT4 AWQ format\n",
    "python3 TensorRT-LLM/examples/quantization/quantize.py \\\n",
    "    --model_dir $HF_MODEL \\\n",
    "    --dtype bfloat16 \\\n",
    "    --qformat int4_awq \\\n",
    "    --awq_block_size 128 \\\n",
    "    --output_dir $CKPT_PATH \\\n",
    "    --calib_size 4\n",
    "\n",
    "trtllm-build --checkpoint_dir $CKPT_PATH \\\n",
    "             --output_dir $ENGINE_PATH \\\n",
    "             --gemm_plugin auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c8c631",
   "metadata": {},
   "source": [
    "### 3. Launch Inference Server\n",
    "\n",
    "Open a terminal and run the following code:\n",
    "\n",
    "- Start the Triton Server with this command:\n",
    "\n",
    "```bash\n",
    "HF_MODEL=Llama-3.1-8B-Instruct\n",
    "ENGINE_PATH=llama31/bf16\n",
    "\n",
    "trtllm-serve $ENGINE_PATH \\\n",
    "--tokenizer $HF_MODEL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35bd27-3f66-4ca4-a645-4931390c1aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl -s http://localhost:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"engine\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Where is New York?\"}\n",
    "        ],\n",
    "        \"max_tokens\": 128,\n",
    "        \"temperature\": 0\n",
    "    }' | jq -r '.choices[0].message.content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b758c-cdf3-41db-abbd-31e1579ed5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
