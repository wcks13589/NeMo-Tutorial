{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27af7c62",
   "metadata": {},
   "source": [
    "# TensorRT-LLM\n",
    "The objective of this notebook is to demonstrate the use of TensorRT-LLM to optimize Llama-3.1-8B-Instruct, run inference, and examine using various advance optimization techniques.\n",
    "\n",
    "## Overview of TensorRT-LLM\n",
    "\n",
    "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that execute those TensorRT engines. It also includes a backend for integration with the NVIDIA Triton Inference Server. Models built with TensorRT-LLM can be executed on a wide range of configurations going from a single GPU to multiple nodes with multiple GPUs (using Tensor Parallelism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a86cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "git clone https://github.com/NVIDIA/TensorRT-LLM.git -b v0.16.0 --single-branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92dc35a",
   "metadata": {},
   "source": [
    "## 1. Download model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fcf797",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "huggingface-cli download meta-llama/Llama-3.1-8B-Instruct --local-dir Llama-3.1-8B-Instruct --local-dir-use-symlinks=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e219a",
   "metadata": {},
   "source": [
    "## 2. Building TensorRT-LLM engine(s) for Llama-3.1-8B-Instruct\n",
    "\n",
    "This section shows how to build tensorrt engine(s) using huggingface model.\n",
    "Before we proceed to build our engine, it is important to be aware of the supported matrixes for Llama-3 as listed below:\n",
    "\n",
    "- FP16\n",
    "- FP8\n",
    "- INT8 & INT4 Weight-Only\n",
    "- SmoothQuant\n",
    "- Groupwise quantization (AWQ/GPTQ)\n",
    "- FP8 KV cache\n",
    "- INT8 KV cache (+ AWQ/per-channel weight-only)\n",
    "- Tensor Parallel\n",
    "\n",
    "### 2.1 Build TensorRT-LLM engines - FP16\n",
    "\n",
    "**TensorRT-LLM** builds TensorRT engine(s) from HF checkpoint. Firstly, we used the `convert_checkpoint.py` script to convert Llama-3-Taiwan-8B-Instruct into tensorrt-llm checkpoint format. We use the `trtllm-build` command to build our tensorrt engine.\n",
    "\n",
    "The `trtllm-build` command builds TensorRT-LLM engines from TensorRT-LLM checkpoints. The checkpoint directory provides the model's weights and architecture configuration. The number of engine files is also same to the number of GPUs used to run inference.\n",
    "\n",
    "`trtllm-build` command has a variety of options. In particular, the plugin-related options have two categories:\n",
    "\n",
    "- Plugin options that requires a data type (e.g., `gpt_attention_plugin`), you can\n",
    "    - explicitly specify `float16`/`bfloat16`/`float32`, so that the plugins are enabled with the specified precision;\n",
    "    - implicitly specify `auto`, so that the plugins are enabled with the precision automatically inferred from model dtype (i.e., the dtype specified in weight conversion); or\n",
    "    - disable the plugin by `disable`.\n",
    "    \n",
    "- Other features that requires a boolean (e.g., `context_fmha`, `paged_kv_cache`, `remove_input_padding`), you can\n",
    "enable/disable the feature by specifying `enable`/`disable`.\n",
    "\n",
    "Normally `trtllm-build` only requires single GPU, but if you've already got all the GPUs needed for inference, you could enable parallel building to make the engine building process faster by adding --workers argument. Please note that currently workers feature only supports single node.\n",
    "\n",
    "The last step is to run the inference using the `run.py` and `summarize.py` script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d81687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT-LLM] TensorRT-LLM version: 0.16.0\n",
      "0.16.0\n",
      "[01/22/2025-17:33:04] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "230it [00:07, 29.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time of reading and converting: 7.903 s\n",
      "Total time of saving checkpoint: 23.163 s\n",
      "Total time of converting checkpoints: 00:00:31\n",
      "[TensorRT-LLM] TensorRT-LLM version: 0.16.0\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set bert_attention_plugin to auto.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set gpt_attention_plugin to auto.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set gemm_plugin to auto.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set gemm_swiglu_plugin to None.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set fp8_rowwise_gemm_plugin to None.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set nccl_plugin to auto.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set lora_plugin to None.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set moe_plugin to auto.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set mamba_conv1d_plugin to auto.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set low_latency_gemm_plugin to None.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set low_latency_gemm_swiglu_plugin to None.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set context_fmha to True.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set bert_context_fmha_fp32_acc to False.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set remove_input_padding to True.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set reduce_fusion to False.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set user_buffer to False.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set tokens_per_block to 64.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set use_paged_context_fmha to False.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set use_fp8_context_fmha to False.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set multiple_profiles to False.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set paged_state to True.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set streamingllm to False.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set use_fused_mlp to True.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [I] Set pp_reduce_scatter to False.\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [W] Implicitly setting LLaMAConfig.fc_after_embed = False\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [W] Implicitly setting LLaMAConfig.use_input_layernorm_in_first_layer = True\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [W] Implicitly setting LLaMAConfig.use_last_layernorm = True\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [W] Implicitly setting LLaMAConfig.layer_idx_offset = 0\n",
      "[01/22/2025-17:33:40] [TRT-LLM] [W] Implicitly setting LLaMAConfig.tie_word_embeddings = False\n",
      "[01/22/2025-17:33:41] [TRT-LLM] [W] Provided but not required tensors: {'embed_positions', 'rotary_inv_freq', 'embed_positions_for_gpt_attention'}\n",
      "[01/22/2025-17:33:41] [TRT-LLM] [I] Set dtype to bfloat16.\n",
      "[01/22/2025-17:33:41] [TRT-LLM] [I] Set paged_kv_cache to True.\n",
      "[01/22/2025-17:33:41] [TRT-LLM] [W] Overriding paged_state to False\n",
      "[01/22/2025-17:33:41] [TRT-LLM] [I] Set paged_state to False.\n",
      "[01/22/2025-17:33:41] [TRT-LLM] [I] max_seq_len is not specified, using deduced value 131072\n",
      "[01/22/2025-17:33:41] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. \n",
      "\n",
      "[01/22/2025-17:33:41] [TRT-LLM] [W] padding removal and fMHA are both enabled, max_input_len is not required and will be ignored\n",
      "[01/22/2025-17:33:43] [TRT] [I] [MemUsageChange] Init CUDA: CPU +15, GPU +0, now: CPU 7407, GPU 423 (MiB)\n",
      "[01/22/2025-17:33:45] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +2038, GPU +374, now: CPU 9601, GPU 797 (MiB)\n",
      "[01/22/2025-17:33:45] [TRT-LLM] [I] Set nccl_plugin to None.\n",
      "[01/22/2025-17:33:45] [TRT-LLM] [I] Total time of constructing network from module object 4.324427843093872 seconds\n",
      "[01/22/2025-17:33:45] [TRT-LLM] [I] Total optimization profiles added: 1\n",
      "[01/22/2025-17:33:45] [TRT-LLM] [I] Total time to initialize the weights in network Unnamed Network 0: 00:00:00\n",
      "[01/22/2025-17:33:45] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0\n",
      "[01/22/2025-17:33:45] [TRT] [W] Unused Input: position_ids\n",
      "[01/22/2025-17:33:45] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[01/22/2025-17:33:45] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[01/22/2025-17:33:45] [TRT] [I] Compiler backend is used during engine build.\n",
      "[01/22/2025-17:33:48] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[01/22/2025-17:33:48] [TRT] [I] Detected 17 inputs and 1 output network tensors.\n",
      "[01/22/2025-17:33:55] [TRT] [I] Total Host Persistent Memory: 100288 bytes\n",
      "[01/22/2025-17:33:55] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[01/22/2025-17:33:55] [TRT] [I] Max Scratch Memory: 160882816 bytes\n",
      "[01/22/2025-17:33:55] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 498 steps to complete.\n",
      "[01/22/2025-17:33:55] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 29.2835ms to assign 19 blocks to 498 nodes requiring 961551360 bytes.\n",
      "[01/22/2025-17:33:55] [TRT] [I] Total Activation Memory: 961550336 bytes\n",
      "[01/22/2025-17:33:55] [TRT] [I] Total Weights Memory: 16127631872 bytes\n",
      "[01/22/2025-17:33:55] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[01/22/2025-17:33:55] [TRT] [I] Engine generation completed in 9.72796 seconds.\n",
      "[01/22/2025-17:33:55] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 15381 MiB\n",
      "[01/22/2025-17:34:01] [TRT-LLM] [I] Total time of building Unnamed Network 0: 00:00:15\n",
      "[01/22/2025-17:34:01] [TRT] [I] Serialized 27 bytes of code generator cache.\n",
      "[01/22/2025-17:34:01] [TRT] [I] Serialized 160925 bytes of compilation cache.\n",
      "[01/22/2025-17:34:01] [TRT] [I] Serialized 8 timing cache entries\n",
      "[01/22/2025-17:34:01] [TRT-LLM] [I] Timing cache serialized to model.cache\n",
      "[01/22/2025-17:34:01] [TRT-LLM] [I] Build phase peak memory: 41202.05 MB, children: 17.73 MB\n",
      "[01/22/2025-17:34:01] [TRT-LLM] [I] Serializing engine to llama31/bf16/rank0.engine...\n",
      "[01/22/2025-17:34:12] [TRT-LLM] [I] Engine serialized. Total time: 00:00:11\n",
      "[01/22/2025-17:34:13] [TRT-LLM] [I] Total time of building all engines: 00:00:32\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Define model weight path, output checkpoint path and output engine path\n",
    "HF_MODEL=Llama-3.1-8B-Instruct\n",
    "CKPT_PATH=ckpt/bf16\n",
    "ENGINE_PATH=llama31/bf16\n",
    "\n",
    "python3 TensorRT-LLM/examples/llama/convert_checkpoint.py \\\n",
    "    --model_dir $HF_MODEL \\\n",
    "    --output_dir $CKPT_PATH \\\n",
    "    --dtype bfloat16 \\\n",
    "    --tp_size 1\n",
    "\n",
    "trtllm-build \\\n",
    "    --checkpoint_dir $CKPT_PATH \\\n",
    "    --output_dir $ENGINE_PATH \\\n",
    "    --gemm_plugin auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f5d329",
   "metadata": {},
   "source": [
    "#### flag description for `convert_checkpoint.py`:\n",
    "- `model_dir`: path to the model directory\n",
    "- `output_dir`: path to the directory to store the tensorrt-llm checkpoint format or the tensorrt engine\n",
    "- `dtype`: data type to use for model conversion to tensorrt-llm checkpoint\n",
    "\n",
    "#### flag description for `trtllm-build`:\n",
    "- `checkpoint_dir`: path to the directory to load the tensorrt-llm checkpoint needed to build the tensorrt engine\n",
    "- `output_dir`: path to the directory to store the tensorrt-llm checkpoint format or the tensorrt engine\n",
    "- `gemm_plugin`: required plugin to prevent accuracy issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667c412",
   "metadata": {},
   "source": [
    "### 2.2 Build TensorRT-LLM engines - INT8 KV cache + per-channel weight-only quantization\n",
    "To maximize performance and reduce memory footprint, TensorRT-LLM allows the models to be executed using different quantization modes. TensorRT-LLM supports INT4 or INT8 weights (and FP16 activations; a.k.a. INT4/INT8 weight-only) as well as a complete implementation of the SmoothQuant technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d28e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install datasets==2.19\n",
    "\n",
    "# Define model weight path, output checkpoint path and output engine path\n",
    "HF_MODEL=Llama-3.1-8B-Instruct\n",
    "CKPT_PATH=ckpt/int8\n",
    "ENGINE_PATH=llama31/int8\n",
    "\n",
    "python3 TensorRT-LLM/examples/llama/convert_checkpoint.py \\\n",
    "    --model_dir $HF_MODEL \\\n",
    "    --output_dir $CKPT_PATH \\\n",
    "    --dtype bfloat16 \\\n",
    "    --tp_size 1 \\\n",
    "    --int8_kv_cache \\\n",
    "    --use_weight_only \\\n",
    "    --weight_only_precision int8\n",
    "\n",
    "trtllm-build \\\n",
    "    --checkpoint_dir $CKPT_PATH \\\n",
    "    --output_dir $ENGINE_PATH \\\n",
    "    --gemm_plugin auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30e439",
   "metadata": {},
   "source": [
    "### 2.3 Build TensorRT-LLM engines - FP8 Post-Training Quantization [Optional]\n",
    "\n",
    "The examples below uses the NVIDIA Modelopt (AlgorithMic Model Optimization) toolkit for the model quantization process. Although the V100 does not support the FP8 datatype, we have included it as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model weight path, output checkpoint path and output engine path\n",
    "HF_MODEL=Llama-3.1-8B-Instruct\n",
    "CKPT_PATH=ckpt/fp8\n",
    "ENGINE_PATH=llama31/fp8\n",
    "\n",
    "python3 TensorRT-LLM/examples/quantization/quantize.py \\\n",
    "    --model_dir $HF_MODEL \\\n",
    "    --dtype bfloat16 \\\n",
    "    --qformat fp8 \\\n",
    "    --kv_cache_dtype fp8 \\\n",
    "    --output_dir $CKPT_PATH \\\n",
    "    --calib_size 512 \\\n",
    "    --tp_size 1\n",
    "\n",
    "\n",
    "trtllm-build --checkpoint_dir $CKPT_PATH \\\n",
    "             --output_dir $ENGINE_PATH \\\n",
    "             --gemm_plugin auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321c7c40",
   "metadata": {},
   "source": [
    "### 2.4 Build TensorRT-LLM engines - Groupwise quantization (AWQ/GPTQ)\n",
    "One can enable AWQ/GPTQ INT4 weight only quantization with these options when building engine with trtllm-build:\n",
    "NVIDIA Modelopt toolkit is used for AWQ weight quantization. Please see [examples/quantization/README.md](tensorrtllm_backend/tensorrt_llm/examples/quantization/README.md) for Modelopt installation instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model weight path, output checkpoint path and output engine path\n",
    "HF_MODEL=Llama-3.1-8B-Instruct\n",
    "CKPT_PATH=ckpt/int4_awq\n",
    "ENGINE_PATH=llama31/int4_awq\n",
    "\n",
    "# Quantize HF LLaMA 8B checkpoint into INT4 AWQ format\n",
    "python3 TensorRT-LLM/examples/quantization/quantize.py \\\n",
    "    --model_dir $HF_MODEL \\\n",
    "    --dtype bfloat16 \\\n",
    "    --qformat int4_awq \\\n",
    "    --awq_block_size 128 \\\n",
    "    --output_dir $CKPT_PATH \\\n",
    "    --calib_size 4\n",
    "\n",
    "trtllm-build --checkpoint_dir $CKPT_PATH \\\n",
    "             --output_dir $ENGINE_PATH \\\n",
    "             --gemm_plugin auto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c8c631",
   "metadata": {},
   "source": [
    "### 3. Launch Inference Server\n",
    "\n",
    "Open a terminal and run the following code:\n",
    "\n",
    "- On the terminal, navigate to the launch script folder by running this command:\n",
    "\n",
    "```bash\n",
    "cd /workspace/\n",
    "```\n",
    "\n",
    "- Start the Triton Server with this command:\n",
    "\n",
    "```bash\n",
    "HF_MODEL=Llama-3.1-8B-Instruct\n",
    "ENGINE_PATH=llama31/bf16\n",
    "\n",
    "trtllm-serve $ENGINE_PATH \\\n",
    "--tokenizer $HF_MODEL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f35bd27-3f66-4ca4-a645-4931390c1aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York is a state located in the northeastern United States. It is one of\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -s http://localhost:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"engine\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Where is New York?\"}\n",
    "        ],\n",
    "        \"max_tokens\": 16,\n",
    "        \"temperature\": 0\n",
    "    }' | jq -r '.choices[0].message.content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b758c-cdf3-41db-abbd-31e1579ed5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
